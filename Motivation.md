# Motivation

A program, as it runs, updates data structures in memory.  For
example, if you call a function, function parameters will be pushed
onto the stack or allocated space in a closure; if you assign to a
variable, some location in memory will be updated.

And then, when you turn off your computer or end the program's
process, those data structures are thrown away.

If you want to *save* your data or execution state, you need to do
that separately.  Save your data to a file, or to a database, or post
it to an API that will save it for you.

And when you run your program again, read it back in again.

As a thought experiment, imagine as your program ran that rather than
updating ephemeral memory, it instead updated the data structures of
the program's execution in a database.

To do this naively would be horrifically, horrendously slow.
However, this is a thought experiment.  Where does it take us?

The most obvious ramification is that your program's data and
execution state would never be lost.  It would always be there,
whenever you turned on your computer or started your program again.

You wouldn't need to save your data yourself, except for when you
wanted to write it out in a form that was easily readable by other
programs or by people.

Another ramification is that execution is no longer restricted to one
program or one computer.


## Concurrency

If we're running a program in the memory space of a single computer,
then concurrent execution is restricted to however many CPU's have
access to that memory: say 4 in a consumer device, perhaps 32 in a
high end server.

However, if the state of execution is being stored in a database (and
the database is capable of supporting that many simultaneous
connections), you can have as many CPU's working on executing your
program as you cared to pay for.

Want to run 200,000 VM instances in the cloud to run your program?
Sure, if you had a database that could handle the load, and if running
your program against a database wasn't so horrifically slow as to make
it pointless.


## Transactions

If we're executing a program concurrently, it's easy to get messed up.

Let's take a super-simple program.

```JavaScript
// In JavaScript
a = a + b;
```

```Scheme
; Or, in Arc, for example
(= a (+ a b))
```

If we're reading and writing these values from a database, our program
might look something like this:

```JavaScript
var a = await read_a();
var b = await read_b();
await store_a(a + b);
```

```Scheme
(store-a (+ (read-a) (read-b)))
```

If our language were reading and writing values to the database
*for* us, we could write our program in the first way, and it would in
effect execute it as if we'd written it in the second way.

So suppose `a` was `5` and `b` was `2` and we executed this code
twice and what we *want* to end up with is `a` equaling `9` (5 + 2 +
2).

The classic problem with concurrency is if we perform these two
executions simultaneously (which we'd _like_ to do if we have 200,000
VM instances running our program, as running more than one instance
doesn't do us much good if only let one instance run our code at the
same time) is that we can get the wrong answer.

One execution can read `a` (`5`), the second can read a (`5`), the
first writes `5 + 2` (`7`), the second writes `5 + 2` (`7`)... and we
end up with `7`.


## Transactions

A convenient solution, when we can, is to use transactions.

Once either the first or the second execution has written the new
value of `a` (`7`), the *other* execution doesn't get to also write
`7` (now the wrong answer), because that execution is based on an
*outdated* value of `a`.

The other execution has to retry, where it will read the *new* value
of `a` (`7`) and write out the correct final value (`9`).

We can't *always* use transactions.  If we have a lot of *contention*,
if *many* executions are trying to update the *same* data, then our
code will start to thrash and spend all its time retrying transactions
instead of making progress.

But what's nice about transactions when we can use them is that we can
treat execution *as if* it were serialized: as if execution was
happening one at a time.


## Mutable and Immutable Data

Usually when people use a database they put all their data in the
database.

Because a database is a "data-base", a base, a place, to store your
data.

So if you have data you put it in your database.

However, *most* of the data usually generated by programs never needs
to get updated.  It's immutable.

A key insight of functional programming languages, popularized by
Clojure, and adopted by libraries such as Immutable and React, is that
often all that's needed is for a *small* amount of mutable data to
refer to a *large* amount of immutable data.

For example, say you have a blog publishing platform and you have a
user editing a 10,000 word blog post.

That's a lot of data, and conventionally that data would go in the
database along with everything else.

However, imagine the user writes a first draft of their post, and
that's version 1.  They make some edits, and that's version 2.  They
make some more changes, and that's version 3.  They publish their
post, and it's version 3 that gets published.

Version 1 of the post, once created, never changes.  Neither does
version 2 or 3.  They're snapshots of the post.  Immutable.

The only part of the data here that *needs* to be mutable is to know
*which* version is the *current* version of the post, and whether it's
been published or not.

If we're using a database for the *purpose* of handling transactions
for us, then the 10,000 words the user has written doesn't need to go
in the database at all.

The immutable data could be stored into Amazon S3, or anywhere
convenient.  Copied, shared, cached, without worrying about whether it
might have gotten out of date or become inconsistent... because the
immutable part of the data doesn't change.

What part of the data *needs* to go in the database, so that we can
use transactions to ensure that concurrency isn't messing up our
program?

```JavaScript
{
  version: 3,
  published: true
}
```

That's it.

## Databases

Most databases that offer serialized transactions only support writes
on a single machine, and most databases which can be scaled up if
needed to run on multiple machines only support reduced consistency
guarantees such "eventual consistency".

Which isn't to say that eventual consistency can't be useful for a
program carefully designed to work around its limitations...

Yet serializable transactions is a simple and powerful method of
dealing with concurrency that "just works" (if you don't have *too*
much contention)... and if you make any mistakes designing your
program for reduced consistency you end up with a system that works
*most* of the time... and on rare, random occasions has a bug.

Which isn't any fun to debug at all.

Especially since the bug isn't reproducible.  You can run your program
*again*, and the bug won't show up, because it only manifests rarely.

Now, if we have some *particular* situation where an eventual
consistency database would be faster and we need that efficiency,
nothing stops us from using an eventual consistency database for that
particular data if we want to.

However by *default* it would be great if we could have serialized
transactions.

[Google Cloud Datastore](https://cloud.google.com/datastore/) is an
intriguing option.  It offers fully serializable transactions, and is
a distributed database that can scale as much as you'd like to pay
for.

There's a catch though: writes to any one object using transactions
are limited to one write per second, and you can't have any more than
25 distinct objects participating in a transaction.

Still, only data that *needs* to be mutable has to be part of the
transaction.  If we're careful not to *unnecessarily* tag data as
mutable (when it doesn't actually get changed), the 25 object limit
might not be too terrible.

There's also [CockroachDB](https://www.cockroachlabs.com/) that offers
fully serializable transactions across a *cluster* of database machines.

And, even a single-machine database like
[PostgreSQL](https://www.postgresql.org/) can handle quite a lot if
we're not unnecessarily burdening it with immutable data that doesn't
*need* to be in the database.
